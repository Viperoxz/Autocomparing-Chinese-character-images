28.2s 1 Available RAM before loading: 30.04 GB
28.2s 2 Using device: cuda
28.4s 3 Loading /kaggle/input/create-trainset/train_pairs.h5 into RAM...
193.8s 4 img1 shape: (586400, 64, 64), min/max: 0.0/1.0
197.5s 5 img2 shape: (586400, 64, 64), min/max: 0.0/1.0
197.5s 6 Loaded in 169.09s
197.5s 7 Loading /kaggle/input/create-trainset/valid_pairs.h5 into RAM...
246.0s 8 img1 shape: (219900, 64, 64), min/max: 0.0/1.0
247.1s 9 img2 shape: (219900, 64, 64), min/max: 0.0/1.0
247.1s 10 Loaded in 49.79s
247.1s 11 Train dataset: 586400 pairs, Ratio of similar pairs: 0.50
247.1s 12 Validation dataset: 219900 pairs, Ratio of similar pairs: 0.50
247.1s 13 Available RAM after loading: 5.20 GB
247.3s 14 
247.3s 15 === Trying lr=0.0005, margin=1.0 ===
790.6s 16 542.5s	Epoch 1/20 - Train Loss: 0.8500, Val Loss: 0.0800
790.6s 17 542.5s	Val loss improved to 0.0800, saving model...
1341.1s 18 550.4s	Epoch 2/20 - Train Loss: 0.0742, Val Loss: 0.0581
1341.1s 19 550.4s	Val loss improved to 0.0581, saving model...
1891.4s 20 550.2s	Epoch 3/20 - Train Loss: 0.0629, Val Loss: 0.0543
1891.4s 21 550.2s	Val loss improved to 0.0543, saving model...
2440.1s 22 548.6s	Epoch 4/20 - Train Loss: 0.0597, Val Loss: 0.0529
2440.1s 23 548.6s	Val loss improved to 0.0529, saving model...
2988.1s 24 547.8s	Epoch 5/20 - Train Loss: 0.0575, Val Loss: 0.0500
2988.1s 25 547.8s	Val loss improved to 0.0500, saving model...
3535.9s 26 547.7s	Epoch 6/20 - Train Loss: 0.0552, Val Loss: 0.0484
3535.9s 27 547.7s	Val loss improved to 0.0484, saving model...
4082.3s 28 546.2s	Epoch 7/20 - Train Loss: 0.0541, Val Loss: 0.0472
4082.3s 29 546.2s	Val loss improved to 0.0472, saving model...
4627.8s 30 545.4s	Epoch 8/20 - Train Loss: 0.0534, Val Loss: 0.0468
4627.8s 31 545.4s	Val loss improved to 0.0468, saving model...
5171.3s 32 543.4s	Epoch 9/20 - Train Loss: 0.0528, Val Loss: 0.0459
5171.3s 33 543.4s	Val loss improved to 0.0459, saving model...
5713.9s 34 542.5s	Epoch 10/20 - Train Loss: 0.0521, Val Loss: 0.0454
5713.9s 35 542.5s	Val loss improved to 0.0454, saving model...
6256.1s 36 542.0s	Epoch 11/20 - Train Loss: 0.0513, Val Loss: 0.0454
6256.1s 37 542.0s	Val loss improved to 0.0454, saving model...
6796.9s 38 540.7s	Epoch 12/20 - Train Loss: 0.0508, Val Loss: 0.0443
6796.9s 39 540.7s	Val loss improved to 0.0443, saving model...
7337.6s 40 540.6s	Epoch 13/20 - Train Loss: 0.0505, Val Loss: 0.0441
7337.6s 41 540.6s	Val loss improved to 0.0441, saving model...
7878.0s 42 540.4s	Epoch 14/20 - Train Loss: 0.0502, Val Loss: 0.0443
8417.5s 43 539.4s	Epoch 15/20 - Train Loss: 0.0499, Val Loss: 0.0430
8417.5s 44 539.4s	Val loss improved to 0.0430, saving model...
8956.9s 45 539.4s	Epoch 16/20 - Train Loss: 0.0497, Val Loss: 0.0430
9496.4s 46 539.4s	Epoch 17/20 - Train Loss: 0.0494, Val Loss: 0.0429
9496.4s 47 539.4s	Val loss improved to 0.0429, saving model...
10035.6s 48 539.0s	Epoch 18/20 - Train Loss: 0.0490, Val Loss: 0.0421
10035.6s 49 539.0s	Val loss improved to 0.0421, saving model...
10574.2s 50 538.5s	Epoch 19/20 - Train Loss: 0.0486, Val Loss: 0.0413
10574.2s 51 538.5s	Val loss improved to 0.0413, saving model...
11113.3s 52 Training lr=0.0005, margin=1.0:   0%|          | 0/20 [00:00<?, ?it/s]Training lr=0.0005, margin=1.0:   5%|▌         | 1/20 [09:02<2:51:49, 542.62s/it]Training lr=0.0005, margin=1.0:  10%|█         | 2/20 [18:13<2:44:10, 547.28s/it]Training lr=0.0005, margin=1.0:  15%|█▌        | 3/20 [27:23<2:35:27, 548.66s/it]Training lr=0.0005, margin=1.0:  20%|██        | 4/20 [36:32<2:26:18, 548.69s/it]Training lr=0.0005, margin=1.0:  25%|██▌       | 5/20 [45:40<2:17:06, 548.43s/it]Training lr=0.0005, margin=1.0:  30%|███       | 6/20 [54:47<2:07:55, 548.22s/it]Training lr=0.0005, margin=1.0:  35%|███▌      | 7/20 [1:03:54<1:58:38, 547.60s/it]Training lr=0.0005, margin=1.0:  40%|████      | 8/20 [1:12:59<1:49:23, 546.95s/it]Training lr=0.0005, margin=1.0:  45%|████▌     | 9/20 [1:22:03<1:40:04, 545.87s/it]Training lr=0.0005, margin=1.0:  50%|█████     | 10/20 [1:31:05<1:30:48, 544.86s/it]Training lr=0.0005, margin=1.0:  55%|█████▌    | 11/20 [1:40:08<1:21:36, 544.03s/it]Training lr=0.0005, margin=1.0:  60%|██████    | 12/20 [1:49:08<1:12:24, 543.05s/it]Training lr=0.0005, margin=1.0:  65%|██████▌   | 13/20 [1:58:09<1:03:16, 542.36s/it]Training lr=0.0005, margin=1.0:  70%|███████   | 14/20 [2:07:10<54:10, 541.76s/it]  Training lr=0.0005, margin=1.0:  75%|███████▌  | 15/20 [2:16:09<45:05, 541.08s/it]Training lr=0.0005, margin=1.0:  80%|████████  | 16/20 [2:25:08<36:02, 540.58s/it]Training lr=0.0005, margin=1.0:  85%|████████▌ | 17/20 [2:34:08<27:00, 540.25s/it]Training lr=0.0005, margin=1.0:  90%|█████████ | 18/20 [2:43:07<17:59, 539.92s/it]Training lr=0.0005, margin=1.0:  95%|█████████▌| 19/20 [2:52:06<08:59, 539.53s/it]Training lr=0.0005, margin=1.0: 100%|██████████| 20/20 [3:01:05<00:00, 539.38s/it]Training lr=0.0005, margin=1.0: 100%|██████████| 20/20 [3:01:05<00:00, 543.26s/it]
11113.3s 53 538.9s	Epoch 20/20 - Train Loss: 0.0483, Val Loss: 0.0410
11113.3s 54 538.9s	Val loss improved to 0.0410, saving model...
11113.3s 55 lr=0.0005, margin=1.0 - Best Val Loss=0.0410
11113.3s 56 
11113.3s 57 === Trying lr=0.001, margin=1.0 ===
11665.3s 58 551.9s	Epoch 1/20 - Train Loss: 0.5676, Val Loss: 0.0789
11665.3s 59 551.9s	Val loss improved to 0.0789, saving model...
12217.1s 60 551.7s	Epoch 2/20 - Train Loss: 0.0723, Val Loss: 0.0606
12217.1s 61 551.7s	Val loss improved to 0.0606, saving model...
12766.8s 62 549.5s	Epoch 3/20 - Train Loss: 0.0650, Val Loss: 0.0573
12766.8s 63 549.5s	Val loss improved to 0.0573, saving model...
13314.6s 64 547.7s	Epoch 4/20 - Train Loss: 0.0624, Val Loss: 0.0559
13314.6s 65 547.7s	Val loss improved to 0.0559, saving model...
13859.8s 66 545.1s	Epoch 5/20 - Train Loss: 0.0613, Val Loss: 0.0552
13859.8s 67 545.1s	Val loss improved to 0.0552, saving model...
14403.4s 68 543.6s	Epoch 6/20 - Train Loss: 0.0610, Val Loss: 0.0559
14945.0s 69 541.6s	Epoch 7/20 - Train Loss: 0.0604, Val Loss: 0.0552
15485.9s 70 540.7s	Epoch 8/20 - Train Loss: 0.0602, Val Loss: 0.0537
15485.9s 71 540.7s	Val loss improved to 0.0537, saving model...
16026.0s 72 540.1s	Epoch 9/20 - Train Loss: 0.0601, Val Loss: 0.0538
16565.0s 73 539.1s	Epoch 10/20 - Train Loss: 0.0600, Val Loss: 0.0539
17103.8s 74 538.7s	Epoch 11/20 - Train Loss: 0.0598, Val Loss: 0.0548
17642.2s 75 538.4s	Epoch 12/20 - Train Loss: 0.0598, Val Loss: 0.0542
18179.1s 76 536.8s	Epoch 13/20 - Train Loss: 0.0572, Val Loss: 0.0505
18179.1s 77 536.8s	Val loss improved to 0.0505, saving model...
18717.7s 78 538.6s	Epoch 14/20 - Train Loss: 0.0563, Val Loss: 0.0506
19255.4s 79 537.6s	Epoch 15/20 - Train Loss: 0.0556, Val Loss: 0.0491
19255.4s 80 537.6s	Val loss improved to 0.0491, saving model...
19793.6s 81 538.0s	Epoch 16/20 - Train Loss: 0.0552, Val Loss: 0.0487
19793.6s 82 538.0s	Val loss improved to 0.0487, saving model...
20331.4s 83 537.7s	Epoch 17/20 - Train Loss: 0.0549, Val Loss: 0.0484
20331.4s 84 537.7s	Val loss improved to 0.0484, saving model...
20869.0s 85 537.5s	Epoch 18/20 - Train Loss: 0.0547, Val Loss: 0.0482
20869.0s 86 537.5s	Val loss improved to 0.0482, saving model...
21406.4s 87 537.4s	Epoch 19/20 - Train Loss: 0.0545, Val Loss: 0.0491
21943.4s 88 537.0s	Epoch 20/20 - Train Loss: 0.0543, Val Loss: 0.0496
21943.4s 89 lr=0.001, margin=1.0 - Best Val Loss=0.0482
21943.4s 90 Training lr=0.001, margin=1.0:   0%|          | 0/20 [00:00<?, ?it/s]Training lr=0.001, margin=1.0:   5%|▌         | 1/20 [09:12<2:54:48, 552.03s/it]Training lr=0.001, margin=1.0:  10%|█         | 2/20 [18:23<2:45:33, 551.89s/it]Training lr=0.001, margin=1.0:  15%|█▌        | 3/20 [27:33<2:36:04, 550.88s/it]Training lr=0.001, margin=1.0:  20%|██        | 4/20 [36:41<2:26:34, 549.69s/it]Training lr=0.001, margin=1.0:  25%|██▌       | 5/20 [45:46<2:17:00, 548.07s/it]Training lr=0.001, margin=1.0:  30%|███       | 6/20 [54:50<2:07:31, 546.56s/it]Training lr=0.001, margin=1.0:  35%|███▌      | 7/20 [1:03:51<1:58:04, 544.94s/it]Training lr=0.001, margin=1.0:  40%|████      | 8/20 [1:12:52<1:48:43, 543.64s/it]Training lr=0.001, margin=1.0:  45%|████▌     | 9/20 [1:21:52<1:39:27, 542.52s/it]Training lr=0.001, margin=1.0:  50%|█████     | 10/20 [1:30:51<1:30:14, 541.45s/it]Training lr=0.001, margin=1.0:  55%|█████▌    | 11/20 [1:39:50<1:21:05, 540.62s/it]Training lr=0.001, margin=1.0:  60%|██████    | 12/20 [1:48:48<1:11:59, 539.95s/it]Training lr=0.001, margin=1.0:  65%|██████▌   | 13/20 [1:57:45<1:02:53, 539.02s/it]Training lr=0.001, margin=1.0:  70%|███████   | 14/20 [2:06:44<53:53, 538.90s/it]  Training lr=0.001, margin=1.0:  75%|███████▌  | 15/20 [2:15:42<44:52, 538.54s/it]Training lr=0.001, margin=1.0:  80%|████████  | 16/20 [2:24:40<35:53, 538.41s/it]Training lr=0.001, margin=1.0:  85%|████████▌ | 17/20 [2:33:38<26:54, 538.25s/it]Training lr=0.001, margin=1.0:  90%|█████████ | 18/20 [2:42:35<17:56, 538.06s/it]Training lr=0.001, margin=1.0:  95%|█████████▌| 19/20 [2:51:33<08:57, 537.87s/it]Training lr=0.001, margin=1.0: 100%|██████████| 20/20 [3:00:30<00:00, 537.60s/it]Training lr=0.001, margin=1.0: 100%|██████████| 20/20 [3:00:30<00:00, 541.51s/it]
21943.5s 91 
21943.5s 92 === Finding best threshold for lr=0.0005, margin=1.0 ===
21943.5s 93 <ipython-input-7-7c76d9864db8>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
21943.5s 94 checkpoint = torch.load(best_checkpoint)
22006.0s 95 Threshold=0.30, Val Accuracy=82.27%
22068.7s 96 Threshold=0.50, Val Accuracy=91.72%
22069.8s 97 
22069.8s 98 === Training and Hyperparameter Tuning Completed ===
22069.8s 99 Best Hyperparameters: lr=0.0005, margin=1.0, threshold=0.5
22069.8s 100 Best Validation Loss: 0.0410, Best Validation Accuracy: 91.72%
22080.3s 101 /usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
22080.3s 102 warn(
22080.4s 103 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook
22080.7s 104 [NbConvertApp] Writing 82062 bytes to __notebook__.ipynb
22082.1s 105 /usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
22082.1s 106 warn(
22082.1s 107 [NbConvertApp] Converting notebook __notebook__.ipynb to html
22083.5s 108 [NbConvertApp] Support files will be in __results___files/
22083.5s 109 [NbConvertApp] Making directory __results___files
22083.5s 110 [NbConvertApp] Writing 365389 bytes to __results__.html